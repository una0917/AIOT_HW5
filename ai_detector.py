import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
import re
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import pickle
import os
from datetime import datetime

# è¨­å®šç¹é«”ä¸­æ–‡å­—é«”
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
# ç°¡æ˜“ä¸­æ–‡å¥å­åˆ‡åˆ†èˆ‡è©å…ƒåŒ–ï¼ˆé¿å…ä¾è³´ NLTK punktï¼‰
def split_sentences(text: str):
    """ä»¥ä¸­æ–‡æ¨™é»æˆ–æ›è¡Œåˆ‡åˆ†å¥å­ï¼Œå»é™¤ç©ºç™½é …ã€‚"""
    # ä½¿ç”¨å¸¸è¦‹ä¸­æ–‡å¥å­çµ‚æ­¢ç¬¦è™Ÿ
    parts = re.split(r'[ã€‚ï¼ï¼Ÿ!?\n]+', text)
    sentences = [p.strip() for p in parts if p and p.strip()]
    return sentences


def tokenize(text: str):
    """ç°¡æ˜“è©å…ƒåŒ–ï¼šæ“·å–é€£çºŒçš„ä¸­æ–‡å­—ç¬¦æˆ–è‹±æ•¸å­—åºåˆ—ä½œç‚º tokenã€‚"""
    # å°ä¸­æ–‡ä½¿ç”¨å–®å­—/é€£çºŒä¸­æ–‡å­—è¦–ç‚º tokenï¼Œè‹±æ–‡æˆ–æ•¸å­—è¦–ç‚ºæ•´ä¸² token
    tokens = re.findall(r'[\u4e00-\u9fff]+|[A-Za-z0-9]+', text)
    return tokens

# ==================== ç‰¹å¾µæå–æ¨¡çµ„ ====================

class AIDetectorFeatureExtractor:
    """
    å¾æ–‡æœ¬ä¸­æå– AI vs Human åˆ†é¡ç‰¹å¾µ
    åŸºæ–¼æ•™å­¸ä¸­çš„ Perplexityã€Burstinessã€Stylometryã€Zipf's Law ç­‰
    """
    
    def __init__(self):
        self.feature_names = [
            'sentence_length_mean',      # å¹³å‡å¥å­é•·åº¦
            'sentence_length_std',       # å¥å­é•·åº¦æ¨™æº–å·®ï¼ˆBurstiness æŒ‡æ¨™ï¼‰
            'burstiness',                # å¥å­ç¯€å¥ (std / mean)
            'type_token_ratio',          # TTR è©å½™å¤šæ¨£æ€§
            'avg_word_length',           # å¹³å‡è©é•·
            'punctuation_ratio',         # æ¨™é»ç¬¦è™Ÿæ¯”ä¾‹
            'function_word_ratio',       # åŠŸèƒ½è©æ¯”ä¾‹
            'comma_ratio',               # é€—è™Ÿæ¯”ä¾‹ï¼ˆæŒ‡ç¤ºè¤‡é›œå¥ï¼‰
            'lexical_diversity',         # è©å½™ä¸é‡è¤‡åº¦
            'entropy_word_freq',         # è©é »ç†µ
            'zipf_tail_ratio',           # Zipf é•·å°¾è©æ¯”ä¾‹
            'repeated_structures',       # é‡è¤‡å¥å¼æ¯”ä¾‹
            'common_connectors_ratio',   # å¸¸è¦‹é€£æ¥è©æ¯”ä¾‹
            'question_mark_ratio',       # å•è™Ÿæ¯”ä¾‹
            'exclamation_ratio',         # é©šå˜†è™Ÿæ¯”ä¾‹
            'passive_voice_indicator',   # è¢«å‹•èªæ…‹æŒ‡æ¨™ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            'avg_entropy_per_sentence',  # æ¯å¥å¹³å‡ç†µ
        ]
    
    def extract_features(self, text):
        """
        å¾æ–‡æœ¬æå–æ‰€æœ‰ç‰¹å¾µ
        """
        features = {}
        
        # åŸºç¤æ¸…ç†
        text = text.strip()
        if len(text) == 0:
            return {name: 0 for name in self.feature_names}
        
        # å¥å­ç´šåˆ¥ç‰¹å¾µ
        sentences = split_sentences(text)
        sentence_lengths = [len(tokenize(sent)) for sent in sentences]
        features['sentence_length_mean'] = np.mean(sentence_lengths) if sentence_lengths else 0
        features['sentence_length_std'] = np.std(sentence_lengths) if sentence_lengths else 0
        
        # Burstiness: å¥å­ç¯€å¥ï¼ˆæ¨™æº–å·® / å¹³å‡ï¼‰
        if features['sentence_length_mean'] > 0:
            features['burstiness'] = features['sentence_length_std'] / features['sentence_length_mean']
        else:
            features['burstiness'] = 0
        
        # è©å½™ç´šåˆ¥ç‰¹å¾µ
        words = tokenize(text.lower())
        words = [w for w in words if re.match(r"[A-Za-z0-9\u4e00-\u9fff]+$", w)]  # åªä¿ç•™ä¸­æ–‡å­—æˆ–è‹±æ•¸
        
        if len(words) > 0:
            unique_words = len(set(words))
            features['type_token_ratio'] = unique_words / len(words)
            features['lexical_diversity'] = unique_words / len(words)
        else:
            features['type_token_ratio'] = 0
            features['lexical_diversity'] = 0
        
        # è©é•·ç‰¹å¾µ
        word_lengths = [len(w) for w in words]
        features['avg_word_length'] = np.mean(word_lengths) if word_lengths else 0
        
        # æ¨™é»ç¬¦è™Ÿç‰¹å¾µ
        total_chars = len(text)
        punctuation_count = sum(1 for c in text if not c.isalnum() and not c.isspace())
        features['punctuation_ratio'] = punctuation_count / total_chars if total_chars > 0 else 0
        
        # ç‰¹æ®Šæ¨™é»
        features['comma_ratio'] = text.count('ï¼Œ') / len(sentences) if len(sentences) > 0 else 0
        features['question_mark_ratio'] = text.count('ï¼Ÿ') / len(sentences) if len(sentences) > 0 else 0
        features['exclamation_ratio'] = text.count('ï¼') / len(sentences) if len(sentences) > 0 else 0
        
        # åŠŸèƒ½è©æ¯”ä¾‹ï¼ˆä¸­æ–‡å¸¸è¦‹åŠŸèƒ½è©ï¼‰
        function_words = ['çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨', 'ä»¥', 'æœ‰', 'ç­‰', 'èˆ‡', 'æˆ–', 'åŠ', 'è€Œ', 'ä½†', 'é€™', 'é‚£', 'å…¶', 'å› æ­¤', 'æ‰€ä»¥', 'å¦‚æœ', 'å°±æ˜¯', 'åªæ˜¯']
        function_word_count = sum(text.count(fw) for fw in function_words)
        features['function_word_ratio'] = function_word_count / len(words) if len(words) > 0 else 0
        
        # å¸¸è¦‹é€£æ¥è©ï¼ˆæ¨¡æ¿åŒ–æŒ‡æ¨™ï¼‰
        common_connectors = ['å› æ­¤', 'ç¸½çµ', 'å€¼å¾—æ³¨æ„', 'å¦å¤–', 'åŒæ™‚', 'æ­¤å¤–', 'æœ€å¾Œ', 'é¦–å…ˆ', 'å…¶æ¬¡', 'ç¸½ä¹‹', 'åŸºæ–¼', 'è€ƒæ…®åˆ°']
        connector_count = sum(text.count(conn) for conn in common_connectors)
        features['common_connectors_ratio'] = connector_count / len(sentences) if len(sentences) > 0 else 0
        
        # Zipf é•·å°¾åˆ†å¸ƒæŒ‡æ¨™
        if len(words) > 0:
            word_freq = Counter(words)
            most_common_freq = word_freq.most_common(1)[0][1] if word_freq else 1
            rare_words = sum(1 for w, freq in word_freq.items() if freq == 1)
            features['zipf_tail_ratio'] = rare_words / len(set(words)) if len(set(words)) > 0 else 0
        else:
            features['zipf_tail_ratio'] = 0
        
        # è©é »ç†µ
        features['entropy_word_freq'] = self._calculate_entropy(words)
        
        # é‡è¤‡å¥å¼ï¼ˆç°¡åŒ–ç‰ˆï¼šé€£çºŒç›¸åŒè©çš„å‡ºç¾ï¼‰
        repeated = 0
        for i in range(len(words) - 1):
            if words[i] == words[i+1]:
                repeated += 1
        features['repeated_structures'] = repeated / len(words) if len(words) > 1 else 0
        
        # è¢«å‹•èªæ…‹æŒ‡æ¨™ï¼ˆç°¡åŒ–ç‰ˆï¼Œè¨ˆç®—ã€Œè¢«ã€å­—å‡ºç¾ç‡ï¼‰
        features['passive_voice_indicator'] = text.count('è¢«') / len(sentences) if len(sentences) > 0 else 0
        
        # æ¯å¥å¹³å‡ç†µ
        sentence_entropies = [self._calculate_entropy(tokenize(s.lower())) for s in sentences if s.strip()]
        features['avg_entropy_per_sentence'] = np.mean(sentence_entropies) if sentence_entropies else 0
        
        return features
    
    def _calculate_entropy(self, words):
        """è¨ˆç®—è©é »ç†µ"""
        if not words:
            return 0
        word_freq = Counter(words)
        total = len(words)
        entropy = 0
        for freq in word_freq.values():
            p = freq / total
            if p > 0:
                entropy -= p * np.log2(p)
        return entropy / np.log2(len(set(words))) if len(set(words)) > 1 else 0  # æ­£è¦åŒ–


# ==================== åˆ†é¡æ¨¡å‹ ====================

class AIDetectorModel:
    """
    AI æ–‡ç« åµæ¸¬æ¨¡å‹
    """
    
    def __init__(self):
        self.extractor = AIDetectorFeatureExtractor()
        self.model = None
        self.model_rf = None
        self.scaler = StandardScaler()
        self.is_trained = False
        self.model_path = "ai_detector_model.pkl"
        self.scaler_path = "ai_detector_scaler.pkl"
    
    def train_sample_model(self):
        """
        è¨“ç·´ä¸€å€‹ç°¡å–®çš„ç¤ºç¯„æ¨¡å‹
        ä½¿ç”¨åˆæˆæ•¸æ“šä¾†å±•ç¤ºåŠŸèƒ½
        """
        # ç”Ÿæˆåˆæˆè¨“ç·´æ•¸æ“š
        n_samples = 100
        features_list = []
        labels = []
        
        # AI ç”Ÿæˆçš„æ–‡æœ¬ç‰¹å¾µï¼ˆç›¸å°ç©©å®šã€é€£æ¥è©å¤šã€å¥é•·å‡å‹»ï¼‰
        for _ in range(n_samples // 2):
            feature_dict = {
                'sentence_length_mean': np.random.normal(15, 3),
                'sentence_length_std': np.random.normal(4, 1),
                'burstiness': np.random.normal(0.25, 0.1),
                'type_token_ratio': np.random.normal(0.6, 0.1),
                'avg_word_length': np.random.normal(3.5, 0.5),
                'punctuation_ratio': np.random.normal(0.08, 0.02),
                'function_word_ratio': np.random.normal(0.25, 0.05),
                'comma_ratio': np.random.normal(0.8, 0.2),
                'lexical_diversity': np.random.normal(0.6, 0.1),
                'entropy_word_freq': np.random.normal(3.5, 0.5),
                'zipf_tail_ratio': np.random.normal(0.35, 0.08),
                'repeated_structures': np.random.normal(0.05, 0.02),
                'common_connectors_ratio': np.random.normal(0.5, 0.1),
                'question_mark_ratio': np.random.normal(0.05, 0.03),
                'exclamation_ratio': np.random.normal(0.02, 0.01),
                'passive_voice_indicator': np.random.normal(0.1, 0.03),
                'avg_entropy_per_sentence': np.random.normal(2.0, 0.3),
            }
            # ç¢ºä¿ç‰¹å¾µå‘é‡çš„æ¬„ä½é †åºèˆ‡ extractor.feature_names å°é½Š
            features_list.append([feature_dict[name] for name in self.extractor.feature_names])
            labels.append(1)  # AI = 1
        
        # Human å¯«çš„æ–‡æœ¬ç‰¹å¾µï¼ˆæ³¢å‹•å¤§ã€é€£æ¥è©å°‘ã€å¥é•·ä¸å‡å‹»ï¼‰
        for _ in range(n_samples // 2):
            feature_dict = {
                'sentence_length_mean': np.random.normal(12, 5),
                'sentence_length_std': np.random.normal(8, 2),
                'burstiness': np.random.normal(0.65, 0.15),
                'type_token_ratio': np.random.normal(0.72, 0.1),
                'avg_word_length': np.random.normal(3.2, 0.6),
                'punctuation_ratio': np.random.normal(0.12, 0.04),
                'function_word_ratio': np.random.normal(0.2, 0.06),
                'comma_ratio': np.random.normal(0.5, 0.3),
                'lexical_diversity': np.random.normal(0.72, 0.1),
                'entropy_word_freq': np.random.normal(4.5, 0.6),
                'zipf_tail_ratio': np.random.normal(0.55, 0.1),
                'repeated_structures': np.random.normal(0.12, 0.05),
                'common_connectors_ratio': np.random.normal(0.15, 0.1),
                'question_mark_ratio': np.random.normal(0.15, 0.08),
                'exclamation_ratio': np.random.normal(0.08, 0.04),
                'passive_voice_indicator': np.random.normal(0.05, 0.03),
                'avg_entropy_per_sentence': np.random.normal(3.5, 0.5),
            }
            features_list.append([feature_dict[name] for name in self.extractor.feature_names])
            labels.append(0)  # Human = 0
        
        X = np.array(features_list)
        y = np.array(labels)
        
        # æ¨™æº–åŒ–
        X_scaled = self.scaler.fit_transform(X)
        
        # è¨“ç·´æ¨¡å‹ï¼šLogistic Regression + Random Forest ä½œç‚º ensemble
        self.model = LogisticRegression(max_iter=1000, random_state=42)
        self.model.fit(X_scaled, y)
        self.model_rf = RandomForestClassifier(n_estimators=200, random_state=42)
        self.model_rf.fit(X_scaled, y)
        self.is_trained = True
    
    def predict(self, text):
        """
        é æ¸¬æ–‡æœ¬æ˜¯å¦ç‚º AI ç”Ÿæˆ
        è¿”å› (AI_probability, Human_probability, è©³ç´°ç‰¹å¾µ)
        """
        if not self.is_trained:
            self.train_sample_model()
        
        # æå–ç‰¹å¾µ
        features_dict = self.extractor.extract_features(text)
        features_array = np.array([features_dict[name] for name in self.extractor.feature_names]).reshape(1, -1)
        
        # æ¨™æº–åŒ–
        features_scaled = self.scaler.transform(features_array)
        
        # é æ¸¬æ¦‚ç‡
        # ä½¿ç”¨ ensembleï¼šLogistic + RandomForest çš„å¹³å‡æ¦‚ç‡
        probs = []
        probs.append(self.model.predict_proba(features_scaled)[0][1])
        if self.model_rf is not None:
            probs.append(self.model_rf.predict_proba(features_scaled)[0][1])
        ai_prob = float(np.mean(probs))

        # è¼•å¾®æ”¶ç¸®æ©Ÿç‡ï¼ˆå‘ 0.5 é æ”ï¼‰ï¼Œé™ä½éåº¦è‡ªä¿¡
        shrink_factor = 0.8  # 0-1ï¼Œè¶Šå°è¶Šä¿å®ˆ
        ai_prob = 0.5 + (ai_prob - 0.5) * shrink_factor

        # é¿å…é¡¯ç¤º 0% æˆ– 100% çš„æ¥µç«¯å€¼
        ai_prob = float(np.clip(ai_prob, 1e-4, 1 - 1e-4))
        human_prob = 1 - ai_prob
        
        return ai_prob, human_prob, features_dict
    
    def get_feature_importance(self):
        """
        ç²å–ç‰¹å¾µé‡è¦æ€§
        """
        if self.model is None:
            return {}
        
        coefficients = self.model.coef_[0]
        importance_dict = {}
        for name, coef in zip(self.extractor.feature_names, coefficients):
            importance_dict[name] = abs(coef)
        
        return importance_dict


# ==================== Streamlit UI ====================

def main():
    st.set_page_config(
        page_title="AI æ–‡ç« åµæ¸¬å™¨",
        page_icon="ğŸ¤–",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # è‡ªè¨‚æ¨£å¼
    st.markdown("""
    <style>
    .main-title {
        text-align: center;
        color: #FF6B6B;
        font-size: 3rem;
        font-weight: bold;
        margin-bottom: 0.5rem;
    }
    .subtitle {
        text-align: center;
        color: #666;
        font-size: 1.1rem;
        margin-bottom: 2rem;
    }
    .result-box-ai {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 20px;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin: 10px 0;
    }
    .result-box-human {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        padding: 20px;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin: 10px 0;
    }
    .metric-card {
        background: #f8f9fa;
        padding: 15px;
        border-radius: 8px;
        border-left: 4px solid #FF6B6B;
        margin: 10px 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # æ¨™é¡Œ
    st.markdown('<div class="main-title">ğŸ¤– AI vs Human æ–‡ç« åµæ¸¬å™¨</div>', unsafe_allow_html=True)
    st.markdown('<div class="subtitle">è¼¸å…¥æ–‡æœ¬ï¼Œç«‹å³åˆ†ææ˜¯å¦ç”± AI ç”Ÿæˆ</div>', unsafe_allow_html=True)
    
    # åˆå§‹åŒ–æ¨¡å‹
    if 'detector_model' not in st.session_state:
        st.session_state.detector_model = AIDetectorModel()
    
    # å´é‚Šæ¬„é…ç½®
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        show_features = st.checkbox("é¡¯ç¤ºè©³ç´°ç‰¹å¾µ", value=True)
        show_visualization = st.checkbox("é¡¯ç¤ºå¯è¦–åŒ–åœ–è¡¨", value=True)
        st.divider()
        st.subheader("ğŸ“– èªªæ˜")
        st.info(
            """
            **åµæ¸¬åŸç†ï¼š**
            - åˆ†æå¥å­ç¯€å¥ï¼ˆBurstinessï¼‰
            - è¨ˆç®—è©å½™å¤šæ¨£æ€§ï¼ˆTTRï¼‰
            - è©•ä¼°å¸¸è¦‹é€£æ¥è©æ¨¡å¼
            - è¨ˆç®—è©é »ç†µï¼ˆEntropyï¼‰
            - è­˜åˆ¥ Zipf é•·å°¾è©æ¯”ä¾‹
            
            **æ³¨æ„ï¼š** é€™æ˜¯è¼”åŠ©å·¥å…·ï¼Œä¸èƒ½ä½œç‚ºå”¯ä¸€åˆ¤æ–·ä¾æ“šã€‚
            """
        )
    
    # ä¸»è¦å…§å®¹å€åŸŸ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“ è¼¸å…¥æ–‡æœ¬")
        text_input = st.text_area(
            "åœ¨ä¸‹æ–¹è¼¸å…¥è¦åˆ†æçš„æ–‡æœ¬ï¼ˆè‡³å°‘ 50 å€‹å­—ï¼‰ï¼š",
            height=250,
            placeholder="ç²˜è²¼è¦åˆ†æçš„æ–‡æœ¬å…§å®¹..."
        )
    
    # åˆ†ææŒ‰éˆ•
    if st.button("ğŸ” ç«‹å³åˆ†æ", key="analyze_btn", use_container_width=True):
        if len(text_input.strip()) < 50:
            st.warning("âš ï¸ è«‹è¼¸å…¥è‡³å°‘ 50 å€‹å­—çš„æ–‡æœ¬")
        else:
            # é€²è¡Œé æ¸¬
            ai_prob, human_prob, features_dict = st.session_state.detector_model.predict(text_input)
            
            # å­˜å„²çµæœåˆ° session
            st.session_state.last_prediction = {
                'ai_prob': ai_prob,
                'human_prob': human_prob,
                'features': features_dict,
                'text': text_input
            }
            
            st.success("âœ… åˆ†æå®Œæˆï¼")
    
    # é¡¯ç¤ºçµæœ
    if 'last_prediction' in st.session_state:
        prediction = st.session_state.last_prediction
        ai_prob = prediction['ai_prob']
        human_prob = prediction['human_prob']
        features_dict = prediction['features']
        
        st.divider()
        st.subheader("ğŸ“Š åˆ†æçµæœ")
        
        # çµæœå±•ç¤º
        col_result1, col_result2 = st.columns(2)
        
        with col_result1:
            st.markdown(f"""
            <div class="result-box-ai">
                <h3>ğŸ¤– AI æ¦‚ç‡</h3>
                <h1>{ai_prob*100:.1f}%</h1>
            </div>
            """, unsafe_allow_html=True)
        
        with col_result2:
            st.markdown(f"""
            <div class="result-box-human">
                <h3>ğŸ‘¤ Human æ¦‚ç‡</h3>
                <h1>{human_prob*100:.1f}%</h1>
            </div>
            """, unsafe_allow_html=True)
        
        # é€²åº¦æ¢è¦–è¦ºåŒ–
        st.subheader("ğŸ“ˆ æ¦‚ç‡åˆ†ä½ˆ")
        col_prob1, col_prob2 = st.columns(2)
        
        with col_prob1:
            st.metric("AI ç”Ÿæˆæ©Ÿç‡", f"{ai_prob*100:.2f}%", 
                     delta=f"{ai_prob*100 - 50:.1f}%" if ai_prob > 0.5 else "")
        
        with col_prob2:
            st.metric("Human æ’°å¯«æ©Ÿç‡", f"{human_prob*100:.2f}%", 
                     delta=f"{human_prob*100 - 50:.1f}%" if human_prob > 0.5 else "")
        
        # ç¹ªè£½é€²åº¦æ¢
        fig, ax = plt.subplots(figsize=(10, 1))
        ax.barh([0], [ai_prob], color='#667eea', label='AI', height=0.5)
        ax.barh([0], [human_prob], left=[ai_prob], color='#f5576c', label='Human', height=0.5)
        ax.set_xlim(0, 1)
        ax.set_ylim(-0.5, 0.5)
        ax.set_xticks([0, 0.25, 0.5, 0.75, 1.0])
        ax.set_xticklabels(['0%', '25%', '50%', '75%', '100%'])
        ax.set_yticks([])
        ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.3), ncol=2)
        ax.set_title('åˆ¤å®šçµæœ', fontweight='bold', pad=10)
        st.pyplot(fig, use_container_width=True)
        
        # è©³ç´°çµ±è¨ˆç‰¹å¾µ
        if show_features:
            st.subheader("ğŸ“‹ è©³ç´°ç‰¹å¾µåˆ†æ")
            
            # å»ºç«‹ç‰¹å¾µè¡¨æ ¼
            feature_df = pd.DataFrame({
                'ç‰¹å¾µåç¨±': st.session_state.detector_model.extractor.feature_names,
                'æ•¸å€¼': [features_dict[name] for name in st.session_state.detector_model.extractor.feature_names]
            })
            
            # åˆ†çµ„é¡¯ç¤º
            col_feat1, col_feat2 = st.columns(2)
            
            with col_feat1:
                st.markdown("**å¥å­ç¯€å¥ç‰¹å¾µ**")
                rhythm_features = {
                    'å¹³å‡å¥é•·': features_dict['sentence_length_mean'],
                    'å¥é•·æ¨™æº–å·®': features_dict['sentence_length_std'],
                    'Burstiness': features_dict['burstiness'],
                }
                for feat_name, feat_val in rhythm_features.items():
                    st.metric(feat_name, f"{feat_val:.3f}")
            
            with col_feat2:
                st.markdown("**è©å½™ç‰¹å¾µ**")
                lexical_features = {
                    'TTR è©å½™å¤šæ¨£æ€§': features_dict['type_token_ratio'],
                    'å¹³å‡è©é•·': features_dict['avg_word_length'],
                    'è©é »ç†µ': features_dict['entropy_word_freq'],
                }
                for feat_name, feat_val in lexical_features.items():
                    st.metric(feat_name, f"{feat_val:.3f}")
            
            col_feat3, col_feat4 = st.columns(2)
            
            with col_feat3:
                st.markdown("**çµæ§‹ç‰¹å¾µ**")
                struct_features = {
                    'åŠŸèƒ½è©æ¯”ä¾‹': features_dict['function_word_ratio'],
                    'å¸¸è¦‹é€£æ¥è©æ¯”ä¾‹': features_dict['common_connectors_ratio'],
                    'Zipf é•·å°¾æ¯”ä¾‹': features_dict['zipf_tail_ratio'],
                }
                for feat_name, feat_val in struct_features.items():
                    st.metric(feat_name, f"{feat_val:.3f}")
            
            with col_feat4:
                st.markdown("**æ¨™é»ç‰¹å¾µ**")
                punct_features = {
                    'æ¨™é»ç¬¦è™Ÿæ¯”ä¾‹': features_dict['punctuation_ratio'],
                    'é€—è™Ÿæ¯”ä¾‹': features_dict['comma_ratio'],
                    'å•è™Ÿæ¯”ä¾‹': features_dict['question_mark_ratio'],
                }
                for feat_name, feat_val in punct_features.items():
                    st.metric(feat_name, f"{feat_val:.3f}")
        
        # å¯è¦–åŒ–åœ–è¡¨
        if show_visualization:
            st.subheader("ğŸ“‰ è¦–è¦ºåŒ–åˆ†æ")
            
            # ç‰¹å¾µé‡è¦æ€§åœ–
            col_viz1, col_viz2 = st.columns(2)
            
            with col_viz1:
                st.markdown("**é—œéµç‰¹å¾µå€¼å°æ¯”**")
                key_features = {
                    'Burstiness': features_dict['burstiness'],
                    'TTR': features_dict['type_token_ratio'],
                    'é€£æ¥è©æ¯”ä¾‹': features_dict['common_connectors_ratio'],
                    'è©é »ç†µ': features_dict['entropy_word_freq'],
                    'Zipfå°¾å·´': features_dict['zipf_tail_ratio'],
                }
                
                fig, ax = plt.subplots(figsize=(8, 5))
                colors = ['#667eea' if features_dict['burstiness'] < 0.4 else '#f5576c' for _ in key_features]
                bars = ax.barh(list(key_features.keys()), list(key_features.values()), color=colors)
                ax.set_xlabel('ç‰¹å¾µå€¼', fontweight='bold')
                ax.set_title('é—œéµç‰¹å¾µå€¼', fontweight='bold')
                plt.tight_layout()
                st.pyplot(fig, use_container_width=True)
            
            with col_viz2:
                st.markdown("**å¥é•·åˆ†å¸ƒåˆ†æ**")
                text = prediction['text']
                sentences = st.session_state.detector_model.extractor.__class__.__bases__[0] if hasattr(st.session_state.detector_model.extractor, '__bases__') else None
                
                # è¨ˆç®—å¥é•·åˆ†å¸ƒ
                sentences = split_sentences(text)
                sentence_lengths = [len(tokenize(sent.lower())) for sent in sentences]
                
                fig, ax = plt.subplots(figsize=(8, 5))
                ax.hist(sentence_lengths, bins=max(5, len(set(sentence_lengths))), 
                       color='#FF6B6B', alpha=0.7, edgecolor='black')
                ax.set_xlabel('å¥é•·ï¼ˆè©æ•¸ï¼‰', fontweight='bold')
                ax.set_ylabel('å‡ºç¾æ¬¡æ•¸', fontweight='bold')
                ax.set_title('å¥é•·åˆ†å¸ƒ', fontweight='bold')
                ax.grid(alpha=0.3)
                plt.tight_layout()
                st.pyplot(fig, use_container_width=True)
        
        # çµè«–
        st.divider()
        st.subheader("ğŸ¯ åˆ¤å®šçµè«–")
        
        if ai_prob > 0.7:
            st.warning(f"""
            **âš ï¸ å¾ˆå¯èƒ½ç‚º AI ç”Ÿæˆ**
            
            è©²æ–‡æœ¬å…·æœ‰ä»¥ä¸‹ AI ç‰¹å¾µï¼š
            - å¥å­ç¯€å¥å¹³ç©©ï¼ˆä½ Burstinessï¼‰
            - å¸¸è¦‹é€£æ¥è©ä½¿ç”¨è¼ƒé »ç¹
            - è©å½™åˆ†å¸ƒè¼ƒè¦å‰‡
            """)
        elif ai_prob > 0.5:
            st.info(f"""
            **âš¡ å¯èƒ½ç‚º AI ç”Ÿæˆæˆ–ç¶“éå¤§å¹…ä¿®æ”¹**
            
            è©²æ–‡æœ¬å±•ç¾äº†æ··åˆç‰¹å¾µï¼Œå»ºè­°äººå·¥å¯©æŸ¥ã€‚
            """)
        else:
            st.success(f"""
            **âœ… å¾ˆå¯èƒ½ç‚º Human æ’°å¯«**
            
            è©²æ–‡æœ¬å…·æœ‰ä»¥ä¸‹äººé¡ç‰¹å¾µï¼š
            - å¥å­é•·åº¦æ³¢å‹•è¼ƒå¤§
            - è©å½™é¸æ“‡å¤šæ¨£æ€§é«˜
            - å­˜åœ¨è‡ªç„¶çš„èªè¨€ä¸è¦å‰‡æ€§
            """)


if __name__ == "__main__":
    main()
